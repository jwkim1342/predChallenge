---
title: "Prediction Challenge"
output: html_notebook
---

# Objective #
A supermarket is offering a new line of organic products.The supermarket's management wants to determine which customers are likely to purchase these products.

The supermarket has a customer loyalty program. As an initial buyer incentive plan, the supermarket provided coupons for the organic products to all of the loyalty program participants and collected data that includes whether these customers purchased any of the organic products.

The ORGANICS data set contains 13 variables and over 22,000 observations. The variables in the data set are  shown below with the appropriate roles and levels:

# Data Preprocessing #
## Data Extraction ##
```{r}
rm(list=ls())
setwd("/Users/jpferrer95/Google Drive/Offline/Spring 2018/BIS 348/Data")
rdata = read.csv("organics.csv")
rdata = rdata[complete.cases(rdata),]
```

In order to focus just on the data we have, and without making any wild assumptions, we have decided to eliminate all entries with missing (NA or Null) values. By doing so we avoid pitfalls in our analysis.After eliminating the empty cells, we can notice the number of entries decreased from 22223 observations to 18904 observations.

## Data Cleaning / Dummies ##
This part of the process is important because we are trying to decide which categorical variables to be taken into account in our predictive model. We gotta create dummy variables for the following variables: Clusters, genders, regions, tv regions and promo class

```{r}
options(scipen=999)

# Cluster Dummies 
rdata$isClusterA = ifelse(rdata$DemClusterGroup == 'A', 1, 0)
rdata$isClusterB = ifelse(rdata$DemClusterGroup == 'B', 1, 0)
rdata$isClusterC = ifelse(rdata$DemClusterGroup == 'C', 1, 0)
rdata$isClusterD = ifelse(rdata$DemClusterGroup == 'D', 1, 0)
rdata$isClusterE = ifelse(rdata$DemClusterGroup == 'E', 1, 0)
rdata$isClusterF = ifelse(rdata$DemClusterGroup == 'F', 1, 0)

# Gender Dummies 
rdata$isMale= ifelse(rdata$DemGender == 'M', 1, 0)
rdata$isFemale = ifelse(rdata$DemGender == 'F', 1, 0)

# Region Dummies
rdata$isMidland = ifelse(rdata$DemReg == 'Midlands', 1, 0)
rdata$isNorth = ifelse(rdata$DemReg == 'North', 1, 0)
rdata$isScottish = ifelse(rdata$DemReg == 'Scottish', 1, 0)
rdata$isSouthEast = ifelse(rdata$DemReg == 'South East', 1, 0)

# TV Region Dummmies
rdata$isBorder = ifelse(rdata$DemTVReg == 'Border', 1, 0)
rdata$isCScotland = ifelse(rdata$DemTVReg == 'C Scotland', 1, 0)
rdata$isEast = ifelse(rdata$DemTVReg == 'East', 1, 0)
rdata$isLondon = ifelse(rdata$DemTVReg == 'London', 1, 0)
rdata$isMidlands = ifelse(rdata$DemTVReg == 'Midlands', 1, 0)
rdata$isNEast = ifelse(rdata$DemTVReg == 'N East', 1, 0)
rdata$isNScot = ifelse(rdata$DemTVReg == 'N Scot', 1, 0)
rdata$isNWest = ifelse(rdata$DemTVReg == 'N West', 1, 0)
rdata$isSSEast = ifelse(rdata$DemTVReg == 'S & S East', 1, 0)
rdata$isSWest= ifelse(rdata$DemTVReg == 'S West', 1, 0)
rdata$isUlster = ifelse(rdata$DemTVReg == 'Ulster', 1, 0)
rdata$isWalesWest = ifelse(rdata$DemTVReg == 'Wales & West', 1, 0)

# Promotion Class Dummies
rdata$isGold = ifelse(rdata$PromClass == 'Gold', 1, 0)
rdata$isPlatinum = ifelse(rdata$PromClass == 'Platinum', 1, 0)
rdata$isSilver = ifelse(rdata$PromClass == 'Silver', 1, 0)
```

The *base case*: U Cluster, Unknown gender, South West region, Yorkshire TV region, Tin promotion class

## Data Partitioning ##
```{r}
cdata = rdata[,-c(1,5,6,7,8,9,13)]
set.seed(123)
ti = sample(nrow(cdata), floor(nrow(cdata)*0.6))
train.df = cdata[ti,]
valid.df = cdata[-ti,]
```

##### Logistic Regression Model ######
# Logistic Regression Model #
## Setting Up Model ##
```{r}
# install.packages("forecast")
# install.packages("caret")
library(forecast)
library(caret)

logReg = glm(TargetBuy~.,data = train.df, family = 'binomial')
logReg.pred = predict(logReg, newdata = valid.df, type = 'response')
pred = ifelse(logReg.pred > 0.5, 1, 0)

pred.df = data.frame(Actual = valid.df$TargetBuy, Prediction = pred,
                     Probability = logReg.pred )

#pred.df[order(-logReg.pred),]
confusionMatrix(pred, valid.df$TargetBuy)
```

## Model Assessment ##
```{r}
# Choose Cutoff #
accT = c()
for(cutoff in seq(0, 1, 0.01)){
    cm = confusionMatrix(ifelse(logReg.pred > cutoff, 1, 0), valid.df$TargetBuy)
    accT = c(accT, cm$overall[1])
}

plot(accT ~ seq(0, 1, 0.01), xlab = "Cutoff Value", ylab = "", type = "l", ylim = c(0,1))
lines(1-accT ~ seq(0,1,0.01), type = "l", lty = 2)
legend("bottomright", c("accuracy", "overall error"), lty = c(1,2), merge = F)
```

*Reduce number of Predictors*
```{r}
# Reduced amount of variables taken into account
# step.logReg = step(logReg, direction = 'both')
step.logReg.pred = predict(step.logReg, newdata = valid.df, type = 'response')

step.pred = ifelse(step.logReg.pred > 0.5, 1, 0)

step.pred.df = data.frame(Actual = valid.df$TargetBuy, Prediction = step.pred,
                          Probability = step.logReg.pred )

#pred.df[order(-logReg.pred),]
confusionMatrix(step.pred, valid.df$TargetBuy)

accuracy(step.logReg.pred,valid.df$TargetBuy)
accuracy(logReg.pred,valid.df$TargetBuy)
summary(step.logReg.pred)
summary(logReg.pred)

# lift chart 
library(gains)
gain.full <- gains(valid.df$TargetBuy,step.logReg.pred, groups=10)
gain.full
data.frame("depth" =  gain.full["depth"], "obs" = gain.full["obs"], "cume.obs" = gain.full["cume.obs"], 
           "mean.resp" = gain.full["mean.resp"], "cume.mean.resp" = gain.full["cume.mean.resp"], "cume.pct.of.total" = gain.full["cume.pct.of.total"], 
           "lift" = gain.full["lift"], "cume.lift" = gain.full["cume.lift"], "mean.prediction" = gain.full["mean.prediction"])
gain.full.origin <- gains(valid.df$TargetBuy,logReg.pred, groups=10)

# plot lift chart
plot(c(0, gain.full$cume.pct.of.total*sum(valid.df$TargetBuy))~c(0,gain.full$cume.obs),
     xlab="# cases", ylab="Cumulative", main="", type="l")
lines(c(0,sum(valid.df$TargetBuy))~c(0, dim(valid.df)[1]), lty=2)
lines(c(0, gain.full.origin$cume.pct.of.total*sum(valid.df$TargetBuy))~c(0,gain.full$cume.obs),col = "red")

#compute deciles and plot decile-wise chart
heights <- gain.full$mean.resp/mean(valid.df$TargetBuy)
midpoints <- barplot(heights, names.arg = gain.full$depth, ylim = c(0,3), 
                     xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")

# ROC curve
library(pROC)
r <- roc(valid.df$TargetBuy, step.logReg.pred)
plot.roc(r)
#compute area under the curve
auc(r)

# ROC curve - Original Model
library(pROC)
r2 <- roc(valid.df$TargetBuy, logReg.pred)
plot.roc(r2)
#compute area under the curve
auc(r2)

```
# Neural Network Model #
## Extra: Data Normalization ##
```{r}
library(neuralnet)
library(forecast)

# Normalize the data
nor = preProcess(cdata, method="range")
nn.data = predict(nor, cdata) 
str(nn.data)

train.df.nn = nn.data[ti,]
valid.df.nn = nn.data[-ti,]
```

## Extra: Data Normalization ##
*Use Stepwise first on LogReg and then use those variables to create the neural network*
*Exhaustive Search*
```{r}

```

# Classification Tree Model #
## Model Creation ##
```{r}
# install.packages(c("rpart", "rpart.plot"))
library(rpart)
library(rpart.plot)

default.ct = rpart(TargetBuy~., data = train.df, method = "class")
prp(default.ct, type = 1, extra = 1, under = TRUE, 
    split.font = 1, varlen = -10, 
    box.col = ifelse(default.ct$frame$var =="<leaf>",     'gray','white'))

default.pred = predict(default.ct, valid.df, type="class")

## Generate confusion matrices for training and validation
library(caret)
confusionMatrix(default.pred, valid.df$TargetBuy)
```

# Conclusion #


*Run* = *Cmd+Shift+Enter*
*Insert Chunk* = *Cmd+Option+I*
*Preview* = *Cmd+Shift+K*

*CLASS NOTES*
*LIFT CHARTS*
The lift chart is basically used in order to see how many cases must be used (customers contacted) using our predictive model and see the benefit in prediction we can have. The lift chart inherently includes the random predictive model which means, if all the cases, or all the people are contacted we will find all the positive cases, but is this efficient. If we can just focus on a number of people, a lift chart is valuable because it allows you to see the which cut off of people should the company go for, without incurring on uneccesary costs or no valuable increase in predictability. Data need to be sorted accordingly to their probability of success, which allows us to focus on the highly predicted customers in our model. 

*Gain Table*
The gains table can show you the depth of the file which constitutes for the percentage of data sample being predicted. The cummulative pct of total response shows the percentage of positive (1) entries that will be explained by the percentage of data being sampled. 

